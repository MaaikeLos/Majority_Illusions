{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import networkx as nx\n",
    "import numpy as np     \n",
    "import pandas as pd\n",
    "from help_functions_drawing import *\n",
    "from help_functions_plots import *\n",
    "from help_functions_misc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\P281866\\\\Documents\\\\PhD\\\\Programming\\\\Majority_Illusion\\\\Github'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.dirname(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Simulation with Erdös-Rényi graphs\n",
    "def run_experiment_ER(min_n, max_n, step_n, nr_epochs, p_blue_values, p_edge_values): \n",
    "    # Creates, for each number of nodes n between [min_n] and [max_n], for each value in [p_edge_values], for each value in [p_blue_values], \n",
    "    # [nr_epochs] Erdös-Renyi graphs with these values for the random parameters, and check on each graph the number of nodes in a majority \n",
    "    # illusion and whether the graph is in a majority-majority illusion. The results are returned in a pandas dataframe.\n",
    "\n",
    "    graph_type = 'ER'\n",
    "    path = os.path.dirname(os.getcwd()) +'/ER_analysis/'\n",
    "    filename = 'minn' + str(min_n) + '_maxn' + str(max_n) + '_step' + str(step_n) + '_epochs' + str(nr_epochs) + '_weak_' + graph_type + '.csv'\n",
    "\n",
    "    node_numbers = range(min_n, max_n+1, step_n)\n",
    "\n",
    "    data = pd.DataFrame(columns=['n', 'p_edge', 'p_blue', 'mi', 'wmi', 'nr_nodes_str_ill', 'nr_nodes_weak_ill', 'deg_assort_coef', 'deg_seq', 'avg_path_length', 'CC', 'EV_centr', 'close_centr', 'between_centr', 'frac_largest_comp', 'tie']) #create empty dataframe\n",
    "    \n",
    "    for n in node_numbers:\n",
    "        print(f'n: {n}')\n",
    "        for p_edge in p_edge_values:\n",
    "            for p_blue in p_blue_values:\n",
    "                for epoch in range(nr_epochs):\n",
    "                    G=nx.erdos_renyi_graph(n, p_edge)\n",
    "                    color_randomly(G, p_blue)\n",
    "                    color_edges(G)\n",
    "                    (has_strict_illusion, has_weak_illusion, nr_str_ill_nodes, nr_weak_ill_nodes, nodes_under_illusion, global_tie) = has_illusions(G)  \n",
    "\n",
    "                ###### Other measures of G:\n",
    "                    deg_assort_coef = nx.degree_assortativity_coefficient(G) #Degree assortativity coefficient\n",
    "                    deg_seq = list(dict(nx.degree(G)).values()) # Degree sequence\n",
    "                    if nx.is_connected(G): #If G is not connected, path length raises error\n",
    "                        avg_path_length = nx.average_shortest_path_length(G) #Average shortest path length\n",
    "                    else:\n",
    "                        avg_path_length = 'NA'\n",
    "                    CC = nx.transitivity(G) #Clustering coefficient\n",
    "\n",
    "                    #Centrality measures:\n",
    "                    try:\n",
    "                        EV_centr = nx.eigenvector_centrality(G, max_iter=500) \n",
    "                        EV_centr = list(EV_centr.values()) #Eigenvector centrality\n",
    "                    except:\n",
    "                        print('EV_centrality failed. Use NA instead.')\n",
    "                        EV_centr = 'NA'\n",
    "\n",
    "                    close_centr = nx.closeness_centrality(G) \n",
    "                    close_centr = list(close_centr.values()) #Closeness centrality\n",
    "                    between_centr = nx.betweenness_centrality(G)\n",
    "                    between_centr = list(between_centr.values()) #Betweenness centrality\n",
    "\n",
    "                    # Fraction of nodes in largest component:\n",
    "                    connected_components = sorted(nx.connected_components(G), key=len, reverse=True)\n",
    "                    G0 = G.subgraph(connected_components[0])\n",
    "                    size_largest_comp = G0.number_of_nodes()\n",
    "                    frac_largest_comp = size_largest_comp / G.number_of_nodes() # Fraction of nodes in largest component\n",
    "\n",
    "                    # Save the new data in the dataframe:\n",
    "                    newrow = pd.DataFrame({'n': [n], 'p_edge': [p_edge], 'p_blue': [p_blue], 'mi': [has_strict_illusion], 'wmi': [has_weak_illusion], 'nr_nodes_str_ill': [nr_str_ill_nodes], 'nr_nodes_weak_ill': [nr_weak_ill_nodes], 'deg_assort_coef': [deg_assort_coef], 'deg_seq': [deg_seq], 'avg_path_length': [avg_path_length], 'CC': [CC], 'EV_centr': [EV_centr], 'close_centr': [close_centr], 'between_centr': [between_centr], 'frac_largest_comp':[frac_largest_comp], 'tie':[global_tie]})\n",
    "                    data = pd.concat([data, newrow], ignore_index=True)\n",
    "\n",
    "    # Save data in csv-file:\n",
    "    data.to_csv(path+filename, header=True, sep=';')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_n = 10\n",
    "max_n = 10\n",
    "step_n = 10\n",
    "nr_epochs = 1\n",
    "p_blue_values = np.arange(0.1, 0.5+0.01, 0.1).tolist()\n",
    "p_blue_values = np.around(p_blue_values, 2) # To change 0.600000001 into 0.6\n",
    "\n",
    "p_edge_values = np.arange(0.1, 0.9+0.01, 0.2).tolist()\n",
    "p_edge_values = np.around(p_edge_values, 2) \n",
    " \n",
    "ER_data = run_experiment_ER(min_n, max_n, step_n, nr_epochs, p_blue_values, p_edge_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate average values for the properties that contain a value per node:\n",
    "\n",
    "# Eigenvector centrality:\n",
    "ER_data['EV_centr'] = ER_data['EV_centr'].apply(to_float_list)\n",
    "average = []\n",
    "for i in range(len(ER_data.EV_centr)):\n",
    "    if ER_data.EV_centr[i]:\n",
    "        average.append(sum(ER_data.EV_centr[i])/len(ER_data.EV_centr[i]))\n",
    "    else:\n",
    "        average.append('NA')\n",
    "ER_data['avg_EV_centr'] = average\n",
    "# Closeness centrality:\n",
    "ER_data['close_centr'] = ER_data['close_centr'].apply(to_float_list)\n",
    "average = []\n",
    "for i in range(len(ER_data.close_centr)):\n",
    "    average.append(sum(ER_data.close_centr[i])/len(ER_data.close_centr[i]))\n",
    "ER_data['avg_close_centr'] = average\n",
    "# Betweenness centrality:\n",
    "ER_data['between_centr'] = ER_data['between_centr'].apply(to_float_list)\n",
    "average = []\n",
    "for i in range(len(ER_data.between_centr)):\n",
    "    average.append(sum(ER_data.between_centr[i])/len(ER_data.between_centr[i]))\n",
    "ER_data['avg_between_centr'] = average\n",
    "# Degree sequence\n",
    "ER_data['deg_seq'] = ER_data['deg_seq'].apply(to_float_list)\n",
    "average = []\n",
    "for i in range(len(ER_data.deg_seq)):\n",
    "    average.append(sum(ER_data.deg_seq[i])/len(ER_data.deg_seq[i]))\n",
    "ER_data['avg_degree'] = average\n",
    "\n",
    "# Save the new data:\n",
    "pathER = os.path.dirname(os.getcwd()) +'/ER_analysis/'\n",
    "filenameER = 'ER_data.csv'\n",
    "ER_data.to_csv(pathER + filenameER, header=True, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Simulation with Holme-Kim graphs\n",
    "def run_experiment_HK(min_n, max_n, step_n, nr_epochs, p_blue_values, p_values): \n",
    "    # Creates, for each number of nodes n between [min_n] and [max_n], for each m in m_values (m is the number of random edges to add for each new node), for each value in [p_values] (p is the probability of \n",
    "    # adding a triangle after adding a random edge), for each value in [p_blue_values], \n",
    "    # [nr_epochs] Holme- Kim powerlaw cluster graphs with these values for the random parameters, and check on each graph the number of nodes in a majority \n",
    "    # illusion and whether the graph is in a majority-majority illusion. The results are returned in a pandas dataframe We also record for each graph certain measures.\n",
    "    # m_values is defined in the loop, because for every n, m should be in the range 1<=m<=n.\n",
    "    # note: if there are no connections, or a complete graph, there is no majority-majority-illusion, so those colums will have only values 0\n",
    "    graph_type = 'HK'\n",
    "    path = os.path.dirname(os.getcwd()) +'/HK_analysis/'\n",
    "    filename = 'minn' + str(min_n) + '_maxn' + str(max_n) + '_step' + str(step_n) + '_epochs' + str(nr_epochs) + '_weak_' + graph_type + 'homophily.csv'\n",
    "\n",
    "    node_numbers = range(min_n, max_n+1, step_n)\n",
    "\n",
    "    # Create empty dataframe:\n",
    "    data = pd.DataFrame(columns=['n', 'm', 'p', 'p_blue','mi', 'wmi', 'nr_nodes_str_ill', 'nr_nodes_weak_ill', 'deg_assort_coef', 'deg_seq', 'avg_path_length', 'CC', 'EV_centr', 'close_centr', 'between_centr', 'frac_largest_comp', 'tie', 'probability_mixed_edge', 'actual_fraction_mixed_edges']) #create empty dataframe\n",
    "    \n",
    "    for n in node_numbers:\n",
    "        print(f'n: {n}')\n",
    "        m_values = [n/10, n/2, 9*n/10] #m is 10, 50, and 90 percent of n\n",
    "        m_values = [int(x) for x in m_values]\n",
    "        for m in m_values:\n",
    "            # print(f'm = {m}')\n",
    "            for p in p_values:\n",
    "                # print(f'p = {p}')\n",
    "                for p_blue in p_blue_values:\n",
    "                    for epoch in range(nr_epochs):\n",
    "                        G=nx.powerlaw_cluster_graph(n, m, p)\n",
    "                        (nr_red_nodes, nr_blue_nodes) = color_randomly(G, p_blue)\n",
    "                        (nr_edges, nr_mixed_edges) = color_edges(G)\n",
    "                        (probability_mixed_edge, actual_fraction_mixed_edges) = homopily(n, nr_red_nodes, nr_blue_nodes, nr_edges, nr_mixed_edges)\n",
    "                        (has_strict_illusion, has_weak_illusion, nr_str_ill_nodes, nr_weak_ill_nodes, nodes_under_illusion, global_tie) = has_illusions(G)\n",
    "\n",
    "                    ###### Other measures of G:\n",
    "                        deg_assort_coef = nx.degree_assortativity_coefficient(G) #Degree assortativity coefficient\n",
    "                        deg_seq = list(dict(nx.degree(G)).values()) # Degree sequence\n",
    "                        if nx.is_connected(G): #If G is not connected, path length raises error\n",
    "                            avg_path_length = nx.average_shortest_path_length(G) #Average shortest path length\n",
    "                        else:\n",
    "                            avg_path_length = 'NA'\n",
    "                        CC = nx.transitivity(G) #Clustering coefficient\n",
    "                        \n",
    "                        #Centrality measures:\n",
    "                        try:\n",
    "                            EV_centr = nx.eigenvector_centrality(G, max_iter=500) \n",
    "                            EV_centr = list(EV_centr.values()) #Eigenvector centrality\n",
    "                        except:\n",
    "                            print('EV_centrality failed. Use NA instead.')\n",
    "                            EV_centr = 'NA'\n",
    "\n",
    "                        close_centr = nx.closeness_centrality(G) \n",
    "                        close_centr = list(close_centr.values()) #Closeness centrality\n",
    "\n",
    "                        between_centr = nx.betweenness_centrality(G)\n",
    "                        between_centr = list(between_centr.values()) #Betweenness centrality\n",
    "\n",
    "                        # Fraction of nodes in largest component:\n",
    "                        connected_components = sorted(nx.connected_components(G), key=len, reverse=True)\n",
    "                        G0 = G.subgraph(connected_components[0])\n",
    "                        size_largest_comp = G0.number_of_nodes()\n",
    "                        frac_largest_comp = size_largest_comp / G.number_of_nodes() # Fraction of nodes in largest component\n",
    "                        \n",
    "                        # Save the new data in the dataframe:\n",
    "                        newrow = pd.DataFrame({'n': [n], 'm': [m], 'p': [p], 'p_blue': [p_blue], 'mi': [has_strict_illusion], 'wmi': [has_weak_illusion], 'nr_nodes_str_ill': [nr_str_ill_nodes], 'nr_nodes_weak_ill': [nr_weak_ill_nodes], 'deg_assort_coef': [deg_assort_coef], 'deg_seq': [deg_seq], 'avg_path_length': [avg_path_length], 'CC': [CC], 'EV_centr': [EV_centr], 'close_centr': [close_centr], 'between_centr': [between_centr], 'frac_largest_comp':[frac_largest_comp], 'tie': [global_tie], 'probability_mixed_edge': [probability_mixed_edge], 'actual_fraction_mixed_edges' : [actual_fraction_mixed_edges]})\n",
    "                        data = pd.concat([data, newrow], ignore_index=True)\n",
    "\n",
    "    data.to_csv(path+filename, header=True, sep=';')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_n = 10\n",
    "max_n = 100\n",
    "step_n = 10\n",
    "nr_epochs = 1000\n",
    "p_blue_values = np.arange(0.1, 0.5+0.01, 0.1).tolist()\n",
    "p_blue_values = np.around(p_blue_values, 2) # To change 0.600000001 into 0.6\n",
    "print(p_blue_values)\n",
    "p_values = np.arange(0, 1+0.01, 0.2).tolist()\n",
    "p_values = np.around(p_values, 2)  \n",
    "\n",
    "HK_data = run_experiment_HK(min_n, max_n, step_n, nr_epochs, p_blue_values, p_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average values for the properties that contain a value per node:\n",
    "# Eigenvector centrality:\n",
    "HK_data['EV_centr'] = HK_data['EV_centr'].apply(to_float_list)\n",
    "average = []\n",
    "for i in range(len(HK_data.EV_centr)):\n",
    "    average.append(sum(HK_data.EV_centr[i])/len(HK_data.EV_centr[i]))\n",
    "HK_data['avg_EV_centr'] = average\n",
    "# Closeness centrality:\n",
    "HK_data['close_centr'] = HK_data['close_centr'].apply(to_float_list)\n",
    "average = []\n",
    "for i in range(len(HK_data.close_centr)):\n",
    "    average.append(sum(HK_data.close_centr[i])/len(HK_data.close_centr[i]))\n",
    "HK_data['avg_close_centr'] = average\n",
    "# Betweenness centrality:\n",
    "HK_data['between_centr'] = HK_data['between_centr'].apply(to_float_list)\n",
    "average = []\n",
    "for i in range(len(HK_data.between_centr)):\n",
    "    average.append(sum(HK_data.between_centr[i])/len(HK_data.between_centr[i]))\n",
    "HK_data['avg_between_centr'] = average\n",
    "# Degree sequence\n",
    "HK_data['deg_seq'] = HK_data['deg_seq'].apply(to_float_list)\n",
    "average = []\n",
    "for i in range(len(HK_data.deg_seq)):\n",
    "    average.append(sum(HK_data.deg_seq[i])/len(HK_data.deg_seq[i]))\n",
    "HK_data['avg_degree'] = average\n",
    "\n",
    "# Save the new data:\n",
    "pathHK = os.path.dirname(os.getcwd()) +'/HK_analysis/'\n",
    "filenameHK = 'HK_data.csv'\n",
    "HK_data.to_csv(pathHK + filenameHK, header=True, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facebook:\n",
    "\n",
    "#Get facebook data: \n",
    "facebook = pd.read_csv(\n",
    "    \"facebook_combined.txt.gz\",\n",
    "    compression=\"gzip\",\n",
    "    sep=\" \",\n",
    "    names=[\"start_node\", \"end_node\"],\n",
    ")\n",
    "G_fb = nx.from_pandas_edgelist(facebook, \"start_node\", \"end_node\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_Facebook(G, nr_epochs, p_blue_values): \n",
    "# Test for majority majority illusion and weak versions on different colorings of a Facebook network. \n",
    "\n",
    "    path = os.path.dirname(os.getcwd()) +'/FB_analysis/'\n",
    "    filename =  'FB_data.csv'\n",
    "\n",
    "    # Create empty dataframe:\n",
    "    data = pd.DataFrame(columns=['p_blue', 'mi', 'wmi', 'nr_nodes_str_ill', 'nr_nodes_weak_ill', 'tie']) #create empty dataframe\n",
    "    \n",
    "    for p_blue in p_blue_values:\n",
    "        print(f'p_blue: {p_blue}') #To keep track of where we are\n",
    "        for epoch in range(nr_epochs):\n",
    "            color_randomly(G, p_blue, start_count_1=True)\n",
    "            color_edges(G)\n",
    "            (has_strict_illusion, has_weak_illusion, nr_str_ill_nodes, nr_weak_ill_nodes, nodes_under_illusion, global_tie) = has_illusions(G)  \n",
    "\n",
    "            # Other measures of G: not necessary since we check only one graph.\n",
    "          \n",
    "            # Save data:\n",
    "            newrow = pd.DataFrame({'p_blue': [p_blue], 'mi': [has_strict_illusion], 'wmi': [has_weak_illusion], 'nr_nodes_str_ill': [nr_str_ill_nodes], 'nr_nodes_weak_ill': [nr_weak_ill_nodes], 'tie':[global_tie]})\n",
    "            data = pd.concat([data, newrow], ignore_index=True)\n",
    "\n",
    "    data.to_csv(path+filename, header=True, sep=';')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_epochs = 1000\n",
    "\n",
    "p_blue_values = np.arange(0.1, 0.5+0.01, 0.1).tolist()\n",
    "p_blue_values = np.around(p_blue_values, 2) # To change 0.600000001 into 0.6\n",
    "\n",
    "FB_data = run_experiment_Facebook(G_fb, nr_epochs, p_blue_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
