{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import networkx as nx\n",
    "import numpy as np     \n",
    "import pandas as pd\n",
    "from help_functions_misc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd() +'/Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Simulation with Holme-Kim graphs\n",
    "def run_experiment_HK(min_n, max_n, step_n, nr_epochs, p_blue_values, p_values): \n",
    "    # Creates, for each number of nodes n between [min_n] and [max_n], for each m in m_values (m is the number of random edges to add for each new node), for each value in [p_values] (p is the probability of \n",
    "    # adding a triangle after adding a random edge), for each value in [p_blue_values], \n",
    "    # [nr_epochs] Holme- Kim powerlaw cluster graphs with these values for the random parameters, and check on each graph the number of nodes in a majority \n",
    "    # illusion and whether the graph is in a majority-majority illusion. The results are returned in a pandas dataframe We also record for each graph certain measures.\n",
    "    # m_values is defined in the loop, because for every n, m should be in the range 1<=m<=n.\n",
    "\n",
    "    graph_type = 'HK'\n",
    "    filename = 'minn' + str(min_n) + '_maxn' + str(max_n) + '_step' + str(step_n) + '_epochs' + str(nr_epochs) + '_' + graph_type + '.csv'\n",
    "\n",
    "    node_numbers = range(min_n, max_n+1, step_n)\n",
    "\n",
    "    # Create empty dataframe:\n",
    "    data = pd.DataFrame(columns=['n', 'm', 'p', 'p_blue', 'proportion_blue_global', 'mi', 'wmi','nr_nodes_str_ill', 'nr_nodes_weak_ill', 'MSE', 'MSE_no_illusion', 'MSE_any_illusion', 'MSE_strict_illusion', 'sum_squared_error_no_illusion', 'sum_squared_error_any_illusion', 'sum_squared_error_strict_illusion', 'deg_assort_coef', 'deg_seq', 'avg_path_length', 'CC', 'EV_centr', 'close_centr', 'between_centr', 'frac_largest_comp','probability_mixed_edge', 'actual_fraction_mixed_edges']) #create empty dataframe\n",
    "\n",
    "    for n in node_numbers:\n",
    "        print(f'n: {n}')\n",
    "        m_values = [n/10, n/2, 9*n/10] #m is 10, 50, and 90 percent of n\n",
    "        m_values = [int(x) for x in m_values]\n",
    "        for m in m_values:\n",
    "            # print(f'm = {m}')\n",
    "            for p in p_values:\n",
    "                # print(f'p = {p}')\n",
    "                for p_blue in p_blue_values:\n",
    "                    for epoch in range(nr_epochs):\n",
    "                        G=nx.powerlaw_cluster_graph(n, m, p)\n",
    "                        (nr_red_nodes, nr_blue_nodes) = color_randomly(G, p_blue)\n",
    "                        (nr_edges, nr_mixed_edges) = color_edges(G)\n",
    "                        (probability_mixed_edge, actual_fraction_mixed_edges) = homopily(n, nr_red_nodes, nr_blue_nodes, nr_edges, nr_mixed_edges)\n",
    "                        (has_strict_illusion, has_weak_illusion, nr_strict_illusion, nr_any_illusion, proportion_blue_global, MSE, MSE_no_illusion, MSE_any_illusion, MSE_strict_illusion, sum_squared_error_no_illusion, sum_squared_error_any_illusion, sum_squared_error_strict_illusion) = check_illusion_and_MSE(G)  \n",
    "\n",
    "                    ###### Other measures of G:\n",
    "                        deg_assort_coef = nx.degree_assortativity_coefficient(G) #Degree assortativity coefficient\n",
    "                        deg_seq = list(dict(nx.degree(G)).values()) # Degree sequence\n",
    "                        if nx.is_connected(G): #If G is not connected, path length raises error\n",
    "                            avg_path_length = nx.average_shortest_path_length(G) #Average shortest path length\n",
    "                        else:\n",
    "                            avg_path_length = 'NA'\n",
    "                        CC = nx.transitivity(G) #Clustering coefficient\n",
    "                        \n",
    "                        #Centrality measures:\n",
    "                        try:\n",
    "                            EV_centr = nx.eigenvector_centrality(G, max_iter=500) \n",
    "                            EV_centr = list(EV_centr.values()) #Eigenvector centrality\n",
    "                        except:\n",
    "                            print('EV_centrality failed. Use NA instead.')\n",
    "                            EV_centr = 'NA'\n",
    "\n",
    "                        close_centr = nx.closeness_centrality(G) \n",
    "                        close_centr = list(close_centr.values()) #Closeness centrality\n",
    "\n",
    "                        between_centr = nx.betweenness_centrality(G)\n",
    "                        between_centr = list(between_centr.values()) #Betweenness centrality\n",
    "\n",
    "                        # Fraction of nodes in largest component:\n",
    "                        connected_components = sorted(nx.connected_components(G), key=len, reverse=True)\n",
    "                        G0 = G.subgraph(connected_components[0])\n",
    "                        size_largest_comp = G0.number_of_nodes()\n",
    "                        frac_largest_comp = size_largest_comp / G.number_of_nodes() # Fraction of nodes in largest component\n",
    "                        \n",
    "                        # Save the new data in the dataframe:\n",
    "                        newrow = pd.DataFrame({'n': [n], 'm': [m], 'p':[p], 'p_blue': [p_blue], 'proportion_blue_global':[proportion_blue_global], 'mi': [has_strict_illusion], 'wmi': [has_weak_illusion], 'nr_nodes_str_ill': [nr_strict_illusion], 'nr_nodes_weak_ill': [nr_any_illusion],  'MSE': [MSE],  'MSE_no_illusion' : [MSE_no_illusion], 'MSE_any_illusion' : [MSE_any_illusion], 'MSE_strict_illusion': [MSE_strict_illusion], 'sum_squared_error_no_illusion': [sum_squared_error_no_illusion], 'sum_squared_error_any_illusion': [sum_squared_error_any_illusion], 'sum_squared_error_strict_illusion': [sum_squared_error_strict_illusion], 'deg_assort_coef': [deg_assort_coef], 'deg_seq': [deg_seq], 'avg_path_length': [avg_path_length], 'CC': [CC], 'EV_centr': [EV_centr], 'close_centr': [close_centr], 'between_centr': [between_centr], 'frac_largest_comp':[frac_largest_comp], 'probability_mixed_edge': [probability_mixed_edge], 'actual_fraction_mixed_edges' : [actual_fraction_mixed_edges]})\n",
    "                        data = pd.concat([data, newrow], ignore_index=True)\n",
    "\n",
    "    data.to_csv(path+filename, header=True, sep=';')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 20\n",
      "n: 40\n",
      "n: 60\n",
      "n: 80\n",
      "n: 100\n"
     ]
    }
   ],
   "source": [
    "#Test run\n",
    "min_n = 20\n",
    "max_n = 100\n",
    "step_n = 20\n",
    "nr_epochs = 2\n",
    "p_blue_values = np.arange(0.1, 0.5+0.01, 0.1).tolist()\n",
    "p_blue_values = np.around(p_blue_values, 2) # To change 0.600000001 into 0.6\n",
    "# print(p_blue_values)\n",
    "p_values = np.arange(0, 1+0.01, 0.2).tolist()\n",
    "p_values = np.around(p_values, 2)  \n",
    "\n",
    "HK_data = run_experiment_HK(min_n, max_n, step_n, nr_epochs, p_blue_values, p_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 20\n",
      "n: 40\n",
      "n: 60\n",
      "n: 80\n",
      "n: 100\n"
     ]
    }
   ],
   "source": [
    "# Even sized graphs\n",
    "min_n = 20\n",
    "max_n = 100\n",
    "step_n = 20\n",
    "nr_epochs = 1000\n",
    "p_blue_values = np.arange(0.1, 0.5+0.01, 0.1).tolist()\n",
    "p_blue_values = np.around(p_blue_values, 2) \n",
    "# print(p_blue_values)\n",
    "p_values = np.arange(0, 1+0.01, 0.2).tolist()\n",
    "p_values = np.around(p_values, 2)  \n",
    "\n",
    "HK_data = run_experiment_HK(min_n, max_n, step_n, nr_epochs, p_blue_values, p_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 21\n",
      "n: 41\n",
      "n: 61\n",
      "n: 81\n",
      "n: 101\n"
     ]
    }
   ],
   "source": [
    "# Odd sized graphs\n",
    "min_n = 21\n",
    "max_n = 101\n",
    "step_n = 20\n",
    "nr_epochs = 1000\n",
    "p_blue_values = np.arange(0.1, 0.5+0.01, 0.1).tolist()\n",
    "p_blue_values = np.around(p_blue_values, 2) \n",
    "p_values = np.arange(0, 1+0.01, 0.2).tolist()\n",
    "p_values = np.around(p_values, 2)  \n",
    "\n",
    "HK_data = run_experiment_HK(min_n, max_n, step_n, nr_epochs, p_blue_values, p_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenameHK_even_read = 'minn20_maxn100_step20_epochs1000_HK.csv'\n",
    "HK_data_even = pd.read_csv(path + filenameHK_even_read, sep=';', index_col=0)\n",
    "# Calculate the global values of centrality:\n",
    "HK_data_even = calculate_aggregated_values(HK_data_even)\n",
    "# Save the new data:\n",
    "filenameHK_even_write = 'HK_data_even.csv'\n",
    "HK_data_even.to_csv(path + filenameHK_even_write, header=True, sep=';')\n",
    "\n",
    "# Same for odd sized graphs:\n",
    "filenameHK_odd_read = 'minn21_maxn101_step20_epochs1000_HK.csv'\n",
    "HK_data_odd = pd.read_csv(path + filenameHK_odd_read, sep=';', index_col=0)\n",
    "HK_data_odd = calculate_aggregated_values(HK_data_odd)\n",
    "filenameHK_odd_write = 'HK_data_odd.csv'\n",
    "HK_data_odd.to_csv(path + filenameHK_odd_write, header=True, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, there are in the Data folder 'HK_data_even.csv' and 'HK_data_odd.csv', which are further dealt with in Data_preprocessing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
